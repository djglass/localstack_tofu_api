# ğŸ§  LocalStack Tofu LLM API

This project builds a fully local, serverless API powered by:

- ğŸ§ª **OpenTofu** â€” infrastructure-as-code
- â˜ï¸ **LocalStack** â€” emulated AWS Lambda + API Gateway
- ğŸ§  **Ollama** â€” local LLM runtime (e.g. Llama3)
- ğŸ **Python** â€” Lambda backend using `requests`

## ğŸš€ Quickstart

### 1. Start LocalStack + Ollama

```bash
localstack start -d
ollama serve &
ollama pull llama3

